import puppeteer, { Browser, Page } from 'puppeteer';
import { Lien } from '../../shared/schema';
// Type definitions
interface County {
  id: string;
  name: string;
  state: string;
  website: string;
  scraperEnabled: boolean;
  searchUrl: string;
  selectors: any;
}

interface CountyConfig {
  url: string;
  searchUrl: string;
  selectors: {
    documentTypeDropdown?: string;
    startDateField?: string;
    endDateField?: string;
    searchButton?: string;
    resultsTable?: string;
  };
}
import { Logger } from './logger';
import { storage } from '../storage';

interface ScrapedLien {
  recordingNumber: string;
  recordingDate: Date;
  debtorName: string;
  debtorAddress: string;
  amount: number;
  creditorName: string;
  creditorAddress: string;
  documentUrl: string;
}

export abstract class CountyScraper {
  constructor(protected county: County, protected config: CountyConfig) {}

  abstract scrapeCountyLiens(startDate?: Date, endDate?: Date): Promise<ScrapedLien[]>;

  async scrapeLiens(): Promise<Lien[]> {
    const scrapedLiens = await this.scrapeCountyLiens();
    const liens: Lien[] = [];

    try {
      // Save liens to storage
      for (const lien of scrapedLiens) {
        liens.push({
          id: crypto.randomUUID(),
          recordingNumber: lien.recordingNumber,
          recordDate: lien.recordingDate,
          countyName: this.county.name,
          countyState: this.county.state,
          debtorName: lien.debtorName,
          debtorAddress: lien.debtorAddress,
          amount: lien.amount.toString(),
          creditorName: lien.creditorName,
          creditorAddress: lien.creditorAddress,
          documentUrl: lien.documentUrl,
          status: 'pending'
        });
      }
      await Logger.success(`Saved ${liens.length} liens from ${this.county.name}`, 'county-scraper');
    } catch (error) {
      await Logger.error(`Failed to save liens from ${this.county.name}: ${error}`, 'county-scraper');
      throw error;
    }
    return liens;
  }
}

export class PuppeteerCountyScraper extends CountyScraper {
  private browser: Browser | null = null;

  async initialize() {
    try {
      this.browser = await puppeteer.launch({
        headless: true,
        executablePath: '/nix/store/zi4f80l169xlmivz8vja8wlphq74qqk0-chromium-125.0.6422.141/bin/chromium',
        args: [
          '--no-sandbox',
          '--disable-setuid-sandbox',
          '--disable-dev-shm-usage',
          '--disable-accelerated-2d-canvas',
          '--disable-gpu',
          '--window-size=1920x1080',
          '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'
        ]
      });
      await Logger.info(`Puppeteer browser initialized for ${this.county.name}`, 'county-scraper');
    } catch (error) {
      await Logger.error(`Failed to initialize browser for ${this.county.name}: ${error}`, 'county-scraper');
      throw error;
    }
  }

  async scrapeCountyLiens(startDate?: Date, endDate?: Date): Promise<ScrapedLien[]> {
    if (!this.browser) {
      await this.initialize();
    }

    const page = await this.browser!.newPage();
    await page.setViewport({ width: 1920, height: 1080 });
    
    const liens: ScrapedLien[] = [];

    try {
      await Logger.info(`Starting lien scraping for ${this.county.name}`, 'county-scraper');

      // Get yesterday's date for the search
      const yesterday = new Date();
      yesterday.setDate(yesterday.getDate() - 1);
      const month = yesterday.getMonth() + 1; // JavaScript months are 0-based
      const day = yesterday.getDate();
      const year = yesterday.getFullYear();
      
      // Build the direct URL with yesterday's date
      const directUrl = `https://legacy.recorder.maricopa.gov/recdocdata/GetRecDataRecentPgDn.aspx?rec=0&suf=&nm=&bdt=${month}%2f${day}%2f${year}&edt=${month}%2f${day}%2f${year}&cde=HL&max=20&res=True&doc1=HL&doc2=&doc3=&doc4=&doc5=`;
      
      await Logger.info(`ðŸ“… Searching for medical liens on ${month}/${day}/${year}`, 'county-scraper');
      await Logger.info(`ðŸ”— Navigating directly to results page`, 'county-scraper');
      
      // Navigate directly to the results page
      await page.goto(directUrl, { 
        waitUntil: 'networkidle2',
        timeout: 30000 
      });

      // Wait for page to load
      await new Promise(resolve => setTimeout(resolve, 3000));

      // Collect all recording numbers from all pages
      const allRecordingNumbers: string[] = [];
      let pageNum = 1;
      let hasNextPage = true;

      while (hasNextPage) {
        await Logger.info(`ðŸ“„ Processing page ${pageNum} of results`, 'county-scraper');

        // Take screenshot for debugging
        await page.screenshot({ path: `results-page-${pageNum}.png` });
        await Logger.info(`ðŸ“¸ Screenshot saved to results-page-${pageNum}.png`, 'county-scraper');
        
        // Extract recording numbers from current page with better debugging
        const pageData = await page.evaluate(() => {
          const numbers: string[] = [];
          const pageInfo: any = {
            url: window.location.href,
            title: document.title,
            bodyText: document.body.innerText?.substring(0, 500) || '',
            tables: document.querySelectorAll('table').length,
            links: []
          };
          
          // Look for the results table
          const tables = document.querySelectorAll('table');
          
          tables.forEach((table, tableIndex) => {
            const rows = table.querySelectorAll('tr');
            
            for (let i = 0; i < Math.min(rows.length, 5); i++) { // Check first 5 rows
              const cells = rows[i].querySelectorAll('td, th');
              const rowData: string[] = [];
              
              cells.forEach((cell, cellIndex) => {
                const cellText = cell.textContent?.trim() || '';
                rowData.push(cellText);
                
                // Look for links in first column
                if (cellIndex === 0) {
                  const link = cell.querySelector('a');
                  if (link) {
                    const linkText = link.textContent?.trim();
                    const href = link.getAttribute('href') || '';
                    
                    pageInfo.links.push({
                      text: linkText,
                      href: href
                    });
                    
                    if (linkText && linkText.match(/^\d{10,12}$/)) {
                      numbers.push(linkText);
                    }
                  } else if (cellText && cellText.match(/^\d{10,12}$/)) {
                    numbers.push(cellText);
                  }
                }
              });
              
              if (i === 0) {
                pageInfo.firstRowContent = rowData;
              }
            }
          });
          
          // Also look for any links with recording numbers anywhere on page
          document.querySelectorAll('a').forEach(link => {
            const text = link.textContent?.trim() || '';
            if (text.match(/^\d{10,12}$/) && !numbers.includes(text)) {
              numbers.push(text);
            }
          });
          
          return { numbers, pageInfo };
        });
        
        const pageRecordingNumbers = pageData.numbers;
        await Logger.info(`ðŸ“Š Page analysis: Tables: ${pageData.pageInfo.tables}, Links found: ${pageData.pageInfo.links.length}`, 'county-scraper');
        if (pageData.pageInfo.firstRowContent) {
          await Logger.info(`First row content: ${JSON.stringify(pageData.pageInfo.firstRowContent)}`, 'county-scraper');
        }
        await Logger.info(`Page snippet: ${pageData.pageInfo.bodyText}`, 'county-scraper');

        await Logger.info(`Found ${pageRecordingNumbers.length} recording numbers on page ${pageNum}`, 'county-scraper');
        allRecordingNumbers.push(...pageRecordingNumbers);

        // Check if there's a "Next Page" button and click it
        hasNextPage = await page.evaluate(() => {
          // Look for next page link/button
          const nextLinks = Array.from(document.querySelectorAll('a, input[type="button"], button'));
          
          for (const link of nextLinks) {
            const text = (link.textContent || (link as HTMLInputElement).value || '').toLowerCase();
            if (text.includes('next') && !text.includes('previous')) {
              // Check if the button/link is disabled
              if ((link as HTMLInputElement).disabled || link.getAttribute('disabled')) {
                return false;
              }
              
              // Click the next button
              (link as HTMLElement).click();
              return true;
            }
          }
          
          return false;
        });

        if (hasNextPage) {
          // Wait for the next page to load
          await new Promise(resolve => setTimeout(resolve, 3000));
          pageNum++;
        }
      }

      await Logger.success(`âœ… Collected ${allRecordingNumbers.length} total recording numbers from ${pageNum} pages`, 'county-scraper');

      // Now process each recording number to get PDF details
      for (const recordingNumber of allRecordingNumbers) {
        try {
          await Logger.info(`ðŸ“‘ Processing recording number: ${recordingNumber}`, 'county-scraper');
          
          // Navigate to the PDF URL
          const pdfUrl = `https://legacy.recorder.maricopa.gov/UnOfficialDocs/pdf/${recordingNumber}.pdf`;
          
          // Create a new page for PDF processing
          const pdfPage = await this.browser!.newPage();
          
          try {
            // Navigate to PDF URL
            const response = await pdfPage.goto(pdfUrl, {
              waitUntil: 'networkidle2',
              timeout: 30000
            });

            if (response?.ok()) {
              // Extract text from PDF (if it's rendered as HTML or has text layer)
              const pageText = await pdfPage.evaluate(() => document.body.innerText || '');
              
              // Parse lien information from the text
              const lienInfo = this.parseLienInfo(pageText, recordingNumber, pdfUrl);
              
              if (lienInfo && lienInfo.amount > 20000) {
                liens.push(lienInfo);
                await Logger.success(`ðŸ’° Found lien over $20,000: ${recordingNumber} - Amount: $${lienInfo.amount}`, 'county-scraper');
              } else if (lienInfo) {
                await Logger.info(`Lien ${recordingNumber} amount ($${lienInfo.amount}) is under $20,000 threshold`, 'county-scraper');
              }
            } else {
              await Logger.warning(`Failed to load PDF for recording number ${recordingNumber}`, 'county-scraper');
            }
          } catch (pdfError) {
            await Logger.warning(`Error processing PDF for ${recordingNumber}: ${pdfError}`, 'county-scraper');
          } finally {
            await pdfPage.close();
          }
          
          // Small delay between PDF requests
          await new Promise(resolve => setTimeout(resolve, 1000));
        } catch (error) {
          await Logger.error(`Error processing recording number ${recordingNumber}: ${error}`, 'county-scraper');
        }
      }

      await Logger.success(`ðŸŽ¯ Found ${liens.length} liens over $20,000 in ${this.county.name}`, 'county-scraper');

    } catch (error) {
      await Logger.error(`Failed to scrape liens from ${this.county.name}: ${error}`, 'county-scraper');
    } finally {
      await page.close();
    }

    return liens;
  }

  private parseLienInfo(text: string, recordingNumber: string, pdfUrl: string): ScrapedLien | null {
    try {
      // Extract amount (look for dollar amounts)
      const amountMatch = text.match(/\$[\d,]+\.?\d*/);
      const amount = amountMatch ? parseFloat(amountMatch[0].replace(/[$,]/g, '')) : 0;
      
      // Extract debtor name (usually after "Debtor:" or similar)
      const debtorMatch = text.match(/(?:Debtor|Patient|Name)[:\s]+([^\n]+)/i);
      const debtorName = debtorMatch ? debtorMatch[1].trim() : 'Unknown';
      
      // Extract debtor address
      const addressMatch = text.match(/(?:Address|Addr)[:\s]+([^\n]+(?:\n[^\n]+)?)/i);
      const debtorAddress = addressMatch ? addressMatch[1].trim().replace(/\n/g, ', ') : 'Unknown';
      
      // Extract creditor information (hospital/medical facility)
      const creditorMatch = text.match(/(?:Creditor|Hospital|Medical|Facility)[:\s]+([^\n]+)/i);
      const creditorName = creditorMatch ? creditorMatch[1].trim() : 'Medical Facility';
      
      // Extract recording date
      const dateMatch = text.match(/(?:Recording Date|Date Recorded|Date)[:\s]+(\d{1,2}\/\d{1,2}\/\d{4})/i);
      const recordingDate = dateMatch ? new Date(dateMatch[1]) : new Date();
      
      return {
        recordingNumber,
        recordingDate,
        debtorName,
        debtorAddress,
        amount,
        creditorName,
        creditorAddress: 'See Document', // Will be in the PDF
        documentUrl: pdfUrl
      };
    } catch (error) {
      Logger.error(`Failed to parse lien info from PDF text: ${error}`, 'county-scraper');
      return null;
    }
  }

  async cleanup() {
    if (this.browser) {
      await this.browser.close();
      await Logger.info(`Browser cleanup completed for ${this.county.name}`, 'county-scraper');
    }
  }
}

// Maricopa County specific implementation
export class MaricopaCountyScraper extends PuppeteerCountyScraper {
  // Uses the base implementation with direct URL approach
}

// Factory function to create appropriate scraper
export function createCountyScraper(county: County, config: CountyConfig): CountyScraper {
  switch (county.name.toLowerCase()) {
    case 'maricopa county':
      return new MaricopaCountyScraper(county, config);
    default:
      return new PuppeteerCountyScraper(county, config);
  }
}